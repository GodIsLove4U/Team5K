{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies.\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure settings for Relational Database Service, and defining database info in config variable.\n",
    "jdbc_url=\"jdbc:postgresql://34.67.52.115/team5k\"\n",
    "config = {'user': 'postgres', \n",
    "          \"password\": \"team5kteam5k\", \n",
    "          \"driver\":\"org.postgresql.Driver\",\n",
    "          \"location\": \"34.67.52.115\",\n",
    "          \"db\": \"team5k\",\n",
    "          \"port\": \"5432\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Flag to print Informational message\n",
    "dmdebug=True\n",
    "\n",
    "### Creating variable holding postgres info in format \"postgres://[user]:[password]@[location]:[port]/[database]\".\n",
    "create_engine_str = ('postgresql://' \n",
    "                     + config[\"user\"] \n",
    "                     + \":\" + config[\"password\"] \n",
    "                     + \"@\" + config[\"location\"] \n",
    "                     + \":\" + config[\"port\"] \n",
    "                     + \"/\" + config[\"db\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Postgres instance.\n",
    "engine = create_engine(create_engine_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['education', 'committee_summary_2020', 'donations', 'agg_county_votes', 'agg_county_donors', 'fec_donor_az', 'health_metrics', 'birth_death_rate', 'postal_codes', 'fec_donor_mi', 'fec_donor_wi', 'fec_committee', 'fec_donor_pa', 'pres_votes_6t', 'unemployment', 'fec_donor_nc', 'fec_donor_fl']\n"
     ]
    }
   ],
   "source": [
    "# Printing info for table names. \n",
    "print (engine.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holding area for function to plot data. (Plot currently not operating <6:00pm 7/31/2020>)\n",
    "def plot_data(X, y, y_pred):\n",
    "    print(\"X=\" + str(len(X)))\n",
    "    print(X)\n",
    "    print(\"y=\" + str(len(y)))\n",
    "    print(y)\n",
    "    print(\"y_pred\" + str(len(y_pred)))\n",
    "    print(y_pred)\n",
    "    \n",
    "    plt.scatter(X, y)\n",
    "    plt.plot(X, y_pred, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function is a definition to take in the votes dataframe. \n",
    "### Function takes in the votes dataframe with 20 years of data, \n",
    "### loops thru every 4 years, runs thru all of the county votes, then returns it in a list. \n",
    "def get_votes_intervals(votes_df, state_po):\n",
    "    if dmdebug==True:\n",
    "        print(\"Running \" + get_votes_intervals.__name__ + \"()...\\n\")\n",
    "    \n",
    "    votes_states_df = votes_df[votes_df['state_po']==state_po]\n",
    "    starting_yr = 2000\n",
    "    ending_yr = 2020\n",
    "    interval = 4\n",
    "    i = starting_yr\n",
    "    \n",
    "    four_yr_dfs = []\n",
    "    while (i <= ending_yr):\n",
    "        votes_states_interval_df = votes_states_df[votes_states_df['year']==i]    \n",
    "        four_yr_dfs.append(votes_states_interval_df)\n",
    "        i += interval\n",
    "        \n",
    "### The list is then aggregated and returned in a dataframe (four_yr_dfs).    \n",
    "    return four_yr_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function is a definition that goes thru each county (string) to pull the election date and calculate votes \n",
    "### in the county that are democrat (blue), republic (red) and other. \n",
    "\n",
    "def vote_distribution(county, election_df, state):\n",
    "    if dmdebug==True:\n",
    "        print(\"Running \" + vote_distribution.__name__ + \"()...\\n\")\n",
    "        \n",
    "    major_parties = [\"democrat\", \"republican\"]\n",
    "        \n",
    "    county_df = election_df[election_df['county']==county]\n",
    "    county_blue_df = county_df[county_df['party']==major_parties[0]]\n",
    "    county_red_df = county_df[county_df['party']==major_parties[1]]  \n",
    "    \n",
    "    #Other = not democratic AND not republican  \n",
    "    other_votes = 0\n",
    "    blue_votes = pd.to_numeric(county_blue_df[\"candidatevotes\"].sum(), errors='coerce')\n",
    "    red_votes = pd.to_numeric(county_red_df[\"candidatevotes\"].sum(), errors='coerce')\n",
    "    \n",
    "    unique_parties = county_df['party'].unique()\n",
    "    for party in unique_parties:\n",
    "        #Get a sum of all non major parties for other category\n",
    "        if party not in major_parties:\n",
    "            party_df = county_df[county_df['party']==party]\n",
    "            other_votes += pd.to_numeric(party_df[\"candidatevotes\"].sum(), errors='coerce')\n",
    "    \n",
    "    ### Total votes is the sum of blue + red + other.\n",
    "    total_votes = blue_votes + red_votes + other_votes\n",
    "    \n",
    "    ### Get the respective percentages of each party, and return values.\n",
    "    percent_blue = (blue_votes / total_votes)\n",
    "    percent_red = (red_votes / total_votes)\n",
    "    percent_other = (other_votes / total_votes)\n",
    "            \n",
    "    percent_dict = {\n",
    "        \"blue_votes\": blue_votes,\n",
    "        \"red_votes\": red_votes,\n",
    "        \"other_votes\": other_votes,\n",
    "        \"total_votes\": total_votes,\n",
    "        \"percent_blue\": percent_blue,\n",
    "        \"percent_red\": percent_red,\n",
    "        \"percent_other\": percent_other,\n",
    "        \"county\": county,\n",
    "        \"state\": state\n",
    "    }\n",
    "    return percent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function is a definition that goes through each county to pull election dates\n",
    "### and calculate sum of donations per party per county.\n",
    "def donor_distribution(county, election_df):    \n",
    "    county = county.strip()\n",
    "    \n",
    "    # Total sum of donations per party per county.\n",
    "    major_parties = [\"democrat\", \"republican\"]\n",
    "        \n",
    "    county_df = election_df[election_df['county']==county]\n",
    "    county_blue_df = county_df[county_df['party']==major_parties[0]]\n",
    "    county_red_df = county_df[county_df['party']==major_parties[1]]  \n",
    "    \n",
    "    ### Other excludes both Republican and Democratic parties from donations per county per party. \n",
    "    other_amt = 0\n",
    "    blue_amt = pd.to_numeric(county_blue_df[\"transaction_amt\"].sum(), errors='coerce')\n",
    "    red_amt = pd.to_numeric(county_red_df[\"transaction_amt\"].sum(), errors='coerce')\n",
    "    \n",
    "    ### TODO enable other amount (Function currently not operating <6:16pm 7/31/2020>)\n",
    "    other_amt =0\n",
    "    #unique_parties = county_df['party'].unique()\n",
    "    #for party in unique_parties:\n",
    "    #    #Get a sum of all non major parties for other category\n",
    "    #    if party not in major_parties:\n",
    "    #        party_df = county_df[county_df['party']==party]\n",
    "    #        other_amt += pd.to_numeric(party_df[\"transaction_amt\"].sum(), errors='coerce')\n",
    "    \n",
    "    ### Total transaction amount is the sum of blue + red + other.\n",
    "    total_amt = blue_amt + red_amt + other_amt\n",
    "    \n",
    "    ### Get the respective percentages.\n",
    "    percent_blue = (blue_amt / total_amt)\n",
    "    percent_red = (red_amt / total_amt)\n",
    "    percent_other = (other_amt / total_amt)\n",
    "            \n",
    "    percent_dict = {\n",
    "        \"blue_amt\": blue_amt,\n",
    "        \"red_amt\": red_amt,\n",
    "        \"other_amt\": other_amt,\n",
    "        \"total_amt\": total_amt,\n",
    "        \"percent_blue\": percent_blue,\n",
    "        \"percent_red\": percent_red,\n",
    "        \"percent_other\": percent_other\n",
    "    }\n",
    "    return percent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.vote_distribution(county, election_df, state)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function is a definition that runs every election year in the loop, gets election dataframes, \n",
    "### and looks for the vote distribution. It will be run on a single state and will return \n",
    "### a dictionary that countains every county in the state as a key. \n",
    "### The value (number of votes) is a dictionary of values.\n",
    "\n",
    "def county_vote_distribution(four_yr_dfs, state):\n",
    "    ### Organizing vote distribution by county.\n",
    "    county_dicts = []\n",
    "    ### Looping through each election dataframe.\n",
    "    for i in range(len(four_yr_dfs)):\n",
    "        county_dict = {}\n",
    "        election_df = four_yr_dfs[i]\n",
    "        unique_counties = election_df[\"county\"].unique()\n",
    "        \n",
    "        ### Looping through each unique county.\n",
    "        for county in unique_counties:\n",
    "            ### Get the percentage of the vote distribution for that county.\n",
    "            percent_dict = vote_distribution(county, election_df, state)\n",
    "            county_dict[county] = percent_dict\n",
    "        county_dicts.append(county_dict)\n",
    "    return county_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nACE\\tAce Party\\t\\nAKI\\tAlaskan Independence Party\\t\\nAIC\\tAmerican Independent Conservative\\t\\nAIP\\tAmerican Independent Party\\t\\nAMP\\tAmerican Party\\t\\nAPF\\tAmerican People's Freedom Party\\t\\nAE\\tAmericans Elect\\t\\nCIT\\tCitizens' Party\\t\\nCMD\\tCommandments Party\\t\\nCMP\\tCommonwealth Party of the U.S.\\t\\nCOM\\tCommunist Party\\t\\nCNC\\tConcerned Citizens Party Of Connecticut\\t\\nCRV\\tConservative Party\\t\\nCON\\tConstitution Party\\t\\nCST\\tConstitutional\\t\\nCOU\\tCountry\\t\\nDCG\\tD.C. Statehood Green Party\\t\\nDNL\\tDemocratic -Nonpartisan League\\t\\nDEM\\tDemocratic Party\\t\\nD/C\\tDemocratic/Conservative\\t\\nDFL\\tDemocratic-Farmer-Labor\\t\\nDGR\\tDesert Green Party\\t\\nFED\\tFederalist\\t\\nFLP\\tFreedom Labor Party\\t\\nFRE\\tFreedom Party\\t\\nGWP\\tGeorge Wallace Party\\t\\nGRT\\tGrassroots\\t\\nGRE\\tGreen Party\\t\\nGR\\tGreen-Rainbow\\t\\nHRP\\tHuman Rights Party\\t\\nIDP\\tIndependence Party\\t\\nIND\\tIndependent\\t\\nIAP\\tIndependent American Party\\t\\nICD\\tIndependent Conservative Democratic\\t\\nIGR\\tIndependent Green\\t\\nIP\\tIndependent Party\\t\\nIDE\\tIndependent Party of Delaware\\t\\nIGD\\tIndustrial Government Party\\t\\nJCN\\tJewish/Christian National\\t\\nJUS\\tJustice Party\\t\\nLRU\\tLa Raza Unida\\tAlso see RUP\\nLBR\\tLabor Party\\tAlso see LAB\\nLFT\\tLess Federal Taxes\\t\\nLBL\\tLiberal Party\\t\\nLIB\\tLibertarian Party\\t\\nLBU\\tLiberty Union Party\\t\\nMTP\\tMountain Party\\t\\nNDP\\tNational Democratic Party\\t\\nNLP\\tNatural Law Party\\t\\nNA\\tNew Alliance\\t\\nNJC\\tNew Jersey Conservative Party\\t\\nNPP\\tNew Progressive Party\\t\\nNPA\\tNo Party Affiliation\\t\\nNOP\\tNo Party Preference\\tCommonly used in CA & WA\\nNNE\\tNone\\t\\nN\\tNonpartisan\\t\\nNON\\tNon-Party\\t\\nOE\\tOne Earth Party\\t\\nOTH\\tOther\\t\\nPG\\tPacific Green\\t\\nPSL\\tParty for Socialism and Liberation\\t\\nPAF\\tPeace And Freedom\\tAlso see PFP\\nPFP\\tPeace And Freedom Party\\tAlso see PAF\\nPFD\\tPeace Freedom Party\\t\\nPOP\\tPeople Over Politics\\t\\nPPY\\tPeople's Party\\t\\nPCH\\tPersonal Choice Party\\t\\nPPD\\tPopular Democratic Party\\t\\nPRO\\tProgressive Party\\t\\nNAP\\tProhibition Party\\t\\nPRI\\tPuerto Rican Independence Party\\t\\nRUP\\tRaza Unida Party\\tAlso see LRU\\nREF\\tReform Party\\t\\nREP\\tRepublican Party\\t\\nRES\\tResource Party\\t\\nRTL\\tRight To Life\\t\\nSEP\\tSocialist Equality Party\\t\\nSLP\\tSocialist Labor Party\\t\\nSUS\\tSocialist Party\\t\\nSOC\\tSocialist Party U.S.A.\\t\\nSWP\\tSocialist Workers Party\\t\\nTX\\tTaxpayers\\t\\nTWR\\tTaxpayers Without Representation\\t\\nTEA\\tTea Party\\t\\nTHD\\tTheo-Democratic\\t\\nLAB\\tU.S. Labor Party\\tAlso see LBR\\nUSP\\tU.S. People's Party\\t\\nUST\\tU.S. Taxpayers Party\\t\\nUN\\tUnaffiliated\\t\\nUC\\tUnited Citizen\\t\\nUNI\\tUnited Party\\t\\nUNK\\tUnknown\\t\\nVET\\tVeterans Party\\t\\nWTP\\tWe the People\\t\\nW\\tWrite-In\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Description of committee party abbreviations. Used to extract democratic and republican info.\n",
    "\n",
    "\"\"\"\n",
    "ACE\tAce Party\t\n",
    "AKI\tAlaskan Independence Party\t\n",
    "AIC\tAmerican Independent Conservative\t\n",
    "AIP\tAmerican Independent Party\t\n",
    "AMP\tAmerican Party\t\n",
    "APF\tAmerican People's Freedom Party\t\n",
    "AE\tAmericans Elect\t\n",
    "CIT\tCitizens' Party\t\n",
    "CMD\tCommandments Party\t\n",
    "CMP\tCommonwealth Party of the U.S.\t\n",
    "COM\tCommunist Party\t\n",
    "CNC\tConcerned Citizens Party Of Connecticut\t\n",
    "CRV\tConservative Party\t\n",
    "CON\tConstitution Party\t\n",
    "CST\tConstitutional\t\n",
    "COU\tCountry\t\n",
    "DCG\tD.C. Statehood Green Party\t\n",
    "DNL\tDemocratic -Nonpartisan League\t\n",
    "DEM\tDemocratic Party\t\n",
    "D/C\tDemocratic/Conservative\t\n",
    "DFL\tDemocratic-Farmer-Labor\t\n",
    "DGR\tDesert Green Party\t\n",
    "FED\tFederalist\t\n",
    "FLP\tFreedom Labor Party\t\n",
    "FRE\tFreedom Party\t\n",
    "GWP\tGeorge Wallace Party\t\n",
    "GRT\tGrassroots\t\n",
    "GRE\tGreen Party\t\n",
    "GR\tGreen-Rainbow\t\n",
    "HRP\tHuman Rights Party\t\n",
    "IDP\tIndependence Party\t\n",
    "IND\tIndependent\t\n",
    "IAP\tIndependent American Party\t\n",
    "ICD\tIndependent Conservative Democratic\t\n",
    "IGR\tIndependent Green\t\n",
    "IP\tIndependent Party\t\n",
    "IDE\tIndependent Party of Delaware\t\n",
    "IGD\tIndustrial Government Party\t\n",
    "JCN\tJewish/Christian National\t\n",
    "JUS\tJustice Party\t\n",
    "LRU\tLa Raza Unida\tAlso see RUP\n",
    "LBR\tLabor Party\tAlso see LAB\n",
    "LFT\tLess Federal Taxes\t\n",
    "LBL\tLiberal Party\t\n",
    "LIB\tLibertarian Party\t\n",
    "LBU\tLiberty Union Party\t\n",
    "MTP\tMountain Party\t\n",
    "NDP\tNational Democratic Party\t\n",
    "NLP\tNatural Law Party\t\n",
    "NA\tNew Alliance\t\n",
    "NJC\tNew Jersey Conservative Party\t\n",
    "NPP\tNew Progressive Party\t\n",
    "NPA\tNo Party Affiliation\t\n",
    "NOP\tNo Party Preference\tCommonly used in CA & WA\n",
    "NNE\tNone\t\n",
    "N\tNonpartisan\t\n",
    "NON\tNon-Party\t\n",
    "OE\tOne Earth Party\t\n",
    "OTH\tOther\t\n",
    "PG\tPacific Green\t\n",
    "PSL\tParty for Socialism and Liberation\t\n",
    "PAF\tPeace And Freedom\tAlso see PFP\n",
    "PFP\tPeace And Freedom Party\tAlso see PAF\n",
    "PFD\tPeace Freedom Party\t\n",
    "POP\tPeople Over Politics\t\n",
    "PPY\tPeople's Party\t\n",
    "PCH\tPersonal Choice Party\t\n",
    "PPD\tPopular Democratic Party\t\n",
    "PRO\tProgressive Party\t\n",
    "NAP\tProhibition Party\t\n",
    "PRI\tPuerto Rican Independence Party\t\n",
    "RUP\tRaza Unida Party\tAlso see LRU\n",
    "REF\tReform Party\t\n",
    "REP\tRepublican Party\t\n",
    "RES\tResource Party\t\n",
    "RTL\tRight To Life\t\n",
    "SEP\tSocialist Equality Party\t\n",
    "SLP\tSocialist Labor Party\t\n",
    "SUS\tSocialist Party\t\n",
    "SOC\tSocialist Party U.S.A.\t\n",
    "SWP\tSocialist Workers Party\t\n",
    "TX\tTaxpayers\t\n",
    "TWR\tTaxpayers Without Representation\t\n",
    "TEA\tTea Party\t\n",
    "THD\tTheo-Democratic\t\n",
    "LAB\tU.S. Labor Party\tAlso see LBR\n",
    "USP\tU.S. People's Party\t\n",
    "UST\tU.S. Taxpayers Party\t\n",
    "UN\tUnaffiliated\t\n",
    "UC\tUnited Citizen\t\n",
    "UNI\tUnited Party\t\n",
    "UNK\tUnknown\t\n",
    "VET\tVeterans Party\t\n",
    "WTP\tWe the People\t\n",
    "W\tWrite-In\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to pull zip code data to combine with selected 6 states.\n",
    "def map_zip_county(unique_zips, state_zips):\n",
    "    county_dict = {}\n",
    "    unique_counties = {}\n",
    "    for zipcode in unique_zips:\n",
    "        county_zip = state_zips[state_zips[\"zip\"] == zipcode]\n",
    "        county_name = county_zip[\"county\"].to_string(index=False).strip()\n",
    "        county_dict[zipcode] = county_name\n",
    "        if county_name not in unique_counties:\n",
    "            unique_counties[county_name] = True\n",
    "            \n",
    "    return (county_dict, unique_counties.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to choose columns and put in new dataframe\n",
    "def select_columns(df, column_names):\n",
    "    new_frame = df.loc[:, column_names]\n",
    "    return new_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition to run encoder on category variables for machine learning processing\n",
    "def one_hot_encode(df):\n",
    "    # Generate our categorical variable list\n",
    "    cat_vars = df.dtypes[df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "    # Create a OneHotEncoder instance\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "    # Fit and transform the OneHotEncoder using the categorical variable list\n",
    "    encode_df = pd.DataFrame(enc.fit_transform(df[cat_vars]))\n",
    "\n",
    "    # Add the encoded variable names to the DataFrame\n",
    "    encode_df.columns = enc.get_feature_names(cat_vars)\n",
    "    \n",
    "    return encode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition to encode all columns with numerical values for machine learning\n",
    "def label_enc(df):\n",
    "    # Create encoder\n",
    "    le = LabelEncoder()\n",
    "    # Encode first DataFrame 1 (where all values are floats)\n",
    "    df = df.apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition to create/define variables to run a linear regression model\n",
    "def run_linear_regression(X, y, x_cols):\n",
    "    # Split the preprocessed data into a training and testing dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78, test_size=0.25)\n",
    "    model = LinearRegression()\n",
    "    #Train the model \n",
    "    model.fit(X_train, y_train)\n",
    "    #Predict the values based on the X test values\n",
    "    \n",
    "    X_test_scaled = MinMaxScaler().fit_transform(X_test)\n",
    "    \n",
    "    print(\"Test Vals!!\")\n",
    "    print(X)\n",
    "    print(y)\n",
    "    #print(X.head())\n",
    "    #print(y.head())\n",
    "    #print(X_test.head())\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(y_pred)\n",
    "        \n",
    "    print(\"Prediction!!\")\n",
    "    print(\"Confusion Matrix!!\")\n",
    "    #y_test = y_test[\"transaction_amt\"].tolist()\n",
    "    y_test = [int(i) for i in y_test]\n",
    "    \n",
    "    # [[1], [2], [3]] => [1,2,3]\n",
    "    #y_pred = numpy.concatenate(y_pred, axis=0 )\n",
    "    #y_pred = y_pred.tolist()\n",
    "    y_pred = [int(i) for i in y_pred]\n",
    "    \n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(matrix)\n",
    "    print(\"Classification Report!!\")\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition to create/define variables to run a logistic regression model\n",
    "def run_logistic_regression(X, y):\n",
    "    # Split the preprocessed data into a training and testing dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78, test_size=0.25, stratify=y)\n",
    "    \n",
    "    classifier = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = classifier.predict(X_test)\n",
    "    \n",
    "    score = accuracy_score(y_test, predictions)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set parameters for linear regression model\n",
    "def run_linear_regression_params(df, sml_params, sml_cols):\n",
    "    run_sml_params(df, sml_params, sml_cols, \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set parameters for logistic regression model\n",
    "def run_logistic_regression_params(df, sml_params, sml_cols):\n",
    "    run_sml_params(df, sml_params, sml_cols, \"logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define and display columns that will be used\n",
    "def run_sml_params(df, sml_params, sml_cols, model_type):\n",
    "    \n",
    "    #Reduce columns to start with\n",
    "    print(df.head())\n",
    "    for sml_param in sml_params:\n",
    "        df_all = select_columns(df, sml_cols)     \n",
    "        \n",
    "        #Run Linear Regression Model on X,y\n",
    "        #Set Y column to just the ML model\n",
    "        y_df = select_columns(df, [sml_param])   \n",
    "        y_df = y_df.fillna(0)\n",
    "        y_df = label_enc(y_df)\n",
    "        y = y_df[sml_param].values\n",
    "        \n",
    "        print(\"Y head\")\n",
    "        print(y)\n",
    "\n",
    "        #Set X Cols to the everything but the parameter to run the ML model\n",
    "        x_cols = sml_cols\n",
    "        x_cols.remove(sml_param)\n",
    "        X_df = select_columns(df, x_cols)\n",
    "        X_df = X_df.fillna(0)\n",
    "        X_df = label_enc(X_df)\n",
    "        X = X_df[x_cols].values\n",
    "        \n",
    "        print(\"X head\")\n",
    "        print(X)\n",
    "        \n",
    "        if model_type == 'linear':\n",
    "            #Run Linear Regresion Model on X,y\n",
    "            run_linear_regression(X, y, x_cols)\n",
    "        elif model_type == 'logistic':\n",
    "            #Run Logistic Regresion Model on X,y\n",
    "            run_logistic_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a new column party to the DF that maps the committee party abbreviation to a major party\n",
    "def merge_cmtid_party(donor_df):        \n",
    "    party_repub = \"republican\"\n",
    "    party_democrat = \"democrat\"\n",
    "        \n",
    "    #Map the affiliation code to the party affiliation\n",
    "    cmte_party_map = {\n",
    "        \"REP\": party_repub,\n",
    "        \"TEA\": party_repub,\n",
    "        \"DNL\": party_democrat,\n",
    "        \"DNL\": party_democrat,\n",
    "        \"DEM\": party_democrat,\n",
    "        \"D/C\": party_democrat,\n",
    "        \"DFL\": party_democrat,\n",
    "        \"THD\": party_democrat,\n",
    "        \"PPD\": party_democrat\n",
    "    }\n",
    "    \n",
    "    donor_df[\"party\"] = donor_df[\"cmte_pty_affiliation\"].map(cmte_party_map)\n",
    "    \n",
    "    return donor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through each of the election year DFs and \n",
    "def donation_county_cycle_distribution(four_yr_dfs, state_zips, committee_df):\n",
    "    if(False):\n",
    "        return [{'Maricopa': {'blue_amt': 1000, 'red_amt': 500, 'other_amt': 50, 'total_amt': 1600, 'percent_blue': .625, 'percent_red': .3125, 'percent_other': .03125}, \n",
    "                 'Pima': {'blue_amt': 2000, 'red_amt': 600, 'other_amt': 10, 'total_amt': 2610, 'percent_blue': .766, 'percent_red': .230, 'percent_other': .004}}]   \n",
    "    \n",
    "    #Organize by county\n",
    "    county_dicts = []\n",
    "    for i in range(len(four_yr_dfs)):\n",
    "        county_dict = {}\n",
    "        election_df = four_yr_dfs[i]\n",
    "        election_df.dropna(subset=[\"zip\"], inplace=True)\n",
    "            \n",
    "        unique_zips = election_df[\"zip\"].unique()\n",
    "        \n",
    "        (zip_county_map, unique_counties) = map_zip_county(unique_zips, state_zips)\n",
    "        \n",
    "        election_df[\"county\"] = election_df[\"zip\"].map(zip_county_map)\n",
    "                \n",
    "        for county in unique_counties:\n",
    "            percent_dict = donor_distribution(county, election_df)\n",
    "            county_dict[county] = percent_dict\n",
    "        county_dicts.append(county_dict)\n",
    "    return county_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a function that removes donor info from 1996-1999\n",
    "def str_dt(donor_date_str):\n",
    "    #01/01/1996 - 12/31/1999\n",
    "    donor_date = datetime.strptime(donor_date_str, '%Y-%m-%d')\n",
    "    return donor_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a function that gets data after 1999\n",
    "def get_year_from_date_str(donor_date_str):\n",
    "    donor_date = str_dt(donor_date_str)\n",
    "    donor_year = donor_date.year\n",
    "    return donor_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look at donor data on presidential election years\n",
    "def get_donors_intervals(donor_df, state):\n",
    "    donors_states_df = donor_df[donor_df['state']==state.lower()]\n",
    "    \n",
    "    starting_yr = 2000\n",
    "    i = starting_yr\n",
    "    interval = 4\n",
    "    prev_year = starting_yr - interval\n",
    "    ending_yr = 2020\n",
    "    \n",
    "    four_yr_dfs = []\n",
    "        \n",
    "    while (i <= ending_yr):\n",
    "        votes_states_interval_df = donors_states_df[(donors_states_df['transaction_dt']>datetime.date(prev_year,1,1)) & (donors_states_df['transaction_dt']<datetime.date(i,3,1))]          \n",
    "        four_yr_dfs.append(votes_states_interval_df)\n",
    "        i += interval\n",
    "        prev_year += interval\n",
    "        \n",
    "    return four_yr_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all donation records for a single state and return it in a dataframe\n",
    "def donor_state_query(state, engine):\n",
    "    #Run queries to get all donation records from the states into dfs\n",
    "    donor_table_name = '\"fec_donor_{}\"'.format(state.lower())    \n",
    "    donor_select_sql = 'select * from {}'.format(donor_table_name)\n",
    "    donor_df = pd.read_sql_query(donor_select_sql,con=engine)\n",
    "    return donor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Machine Learning models run on the Voter data - linear regression\n",
    "def votes_linear_regression(votes_df):    \n",
    "    sml_params = [\"blue_votes\", \"red_votes\", \"other_votes\", \"blue_amt\", \"red_amt\", \"other_amt\"]\n",
    "    sml_cols = [\"blue_votes\", \"red_votes\", \"other_votes\", \"blue_amt\", \"red_amt\", \"other_amt\", \"state\", \"county\"]\n",
    "    \n",
    "    run_linear_regression_params(votes_df, sml_params, sml_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Machine Learning models run on the donation data - logistic regression\n",
    "def donation_logistic_regression(donor_df):\n",
    "    sml_params = [\"party\"]\n",
    "    sml_cols = [\"cmt_id\", \"city\", \"state\", \"zip\", \"employer\", \"occupation\", \"transaction_amt\", \"party\"]\n",
    "    \n",
    "    #Run logistic regression to test if we can classify the party\n",
    "    run_logistic_regression_params(donor_df, sml_params, sml_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Machine Learning models run on the donation data - linear regression\n",
    "def donation_linear_regression(donor_df):\n",
    "    sml_params = [\"transaction_amt\", \"employer\", \"occupation\"]\n",
    "    #sml_cols = [\"cmt_id\", \"city\", \"state\", \"zip\", \"employer\", \"occupation\", \"transaction_amt\", \"party\"]\n",
    "    sml_cols = [\"cmt_id\", \"city\", \"state\", \"zip\", \"employer\", \"occupation\", \"transaction_amt\"]\n",
    "    \n",
    "    #Run some machine learning models on the donation of the state\n",
    "    run_linear_regression_params(donor_df, sml_params, sml_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Machine Learning models run on the election data - linear regression\n",
    "def predict_votes_linear_regression(state_model_dict):\n",
    "    election_yr = 2000\n",
    "    unemployment_df = pd.read_sql_query('select * from \"unemployment\"',con=engine)\n",
    "    education_df = pd.read_sql_query('select * from \"education\"',con=engine)\n",
    "    birth_death_df = pd.read_sql_query('select * from \"birth_death_rate\"',con=engine)\n",
    "    for state in state_model_dict.keys():\n",
    "        model = state_model_dict[state]\n",
    "        #TODO enable prediction for voting\n",
    "        #state_sml(model, state, election_yr, unemployment_df, education_df, birth_death_df)\n",
    "        election_yr += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Loop of the program.\n",
    "def main():\n",
    "    ### Set information level\n",
    "    ###dmdebug = True\n",
    "    \n",
    "    #Read the various tables into DFs\n",
    "    health_df = pd.read_sql_query('select * from \"health_metrics\"',con=engine)\n",
    "    committee_df = pd.read_sql_query('select * from \"fec_committee\"',con=engine)\n",
    "    votes_df = pd.read_sql_query('select * from \"pres_votes_6t\"',con=engine)\n",
    "    zips_df = pd.read_sql_query('select * from \"postal_codes\"',con=engine)\n",
    "    \n",
    "    #Lowercase the column\n",
    "    committee_df['cmte_id'] = committee_df['cmte_id'].str.lower()\n",
    "    \n",
    "    #List of swing states to run the analysis on\n",
    "    supported_states = [\"AZ\", \"MI\", \"FL\", \"NC\", \"PA\", \"WI\"]\n",
    "    \n",
    "    #Loop through each state\n",
    "    state_model_dict = {}\n",
    "    for state in supported_states:\n",
    "        #Get the votes related to that state\n",
    "        votes_intervals_df = get_votes_intervals(votes_df, state)\n",
    "\n",
    "        #Get the distribution of Red, Blue, and Other votes in a list of dict per election yr e.g. 2000 + 4n\n",
    "        counties_votes_dicts = county_vote_distribution(votes_intervals_df, state)\n",
    "        #print(counties_votes_dicts)\n",
    "        \n",
    "        #DF that has all donation for a state\n",
    "        donor_df_orig = donor_state_query(state, engine)\n",
    "        #Add party column to donor data frame\n",
    "        donor_df = committee_df.merge(donor_df_orig, left_on='cmte_id', right_on='cmt_id')\n",
    "        \n",
    "        #TODO before merging the party, we need to add the party code to the columns.\n",
    "        donor_df = merge_cmtid_party(donor_df)\n",
    "        \n",
    "        #Run the machine learning models on the donation set\n",
    "        donation_linear_regression(donor_df)\n",
    "        \n",
    "        #TODO Once we have full dataset, then enable logistic regression\n",
    "        #donation_logistic_regression(donor_df)\n",
    "        \n",
    "        #Get a list of DFs that for election election year for that state\n",
    "        donors_intervals_df = get_donors_intervals(donor_df, state)\n",
    "        #Filter out the zips DF by the state\n",
    "        state_zips = zips_df[zips_df[\"state\"] == state]\n",
    "        #Get list of dictionaries \n",
    "        donor_dicts = donation_county_cycle_distribution(donors_intervals_df, state_zips, committee_df)\n",
    "        #Set a tuple to pass to the functions to run machine learning\n",
    "        state_tuple = (counties_votes_dicts, donor_dicts)\n",
    "\n",
    "        state_model_dict[state] = state_tuple\n",
    "        \n",
    "        #TODO enable the neural networking code\n",
    "        state_nn(state_tuple)\n",
    "    \n",
    "    #TODO: Now with all states donations and voting results aggregated, predict the number of votes\n",
    "    #predict_votes_linear_regression(state_model_dict)\n",
    "    \n",
    "    #TODO: Run Linear regression on the votes\n",
    "    #votes_linear_regression(votes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to loop thru voter info and calculate red, blue, other \n",
    "def set_votes_dict(votes_dict, county_votes, donor_dict):\n",
    "    votes_dict[\"blue_votes\"] = county_votes[\"blue_votes\"]\n",
    "    votes_dict[\"red_votes\"] = county_votes[\"red_votes\"]\n",
    "    votes_dict[\"other_votes\"] = county_votes[\"other_votes\"]\n",
    "    votes_dict[\"state\"] = county_votes[\"state\"]\n",
    "    votes_dict[\"county\"] = county_votes[\"county\"]\n",
    "\n",
    "    for donor_c in donor_dict:\n",
    "        if c == donor_c:\n",
    "            county_donors = donor_dict[donor_c]\n",
    "            votes_dict[\"blue_amt\"] = county_donors[\"blue_amt\"]\n",
    "            votes_dict[\"red_amt\"] = county_donors[\"red_amt\"]\n",
    "            votes_dict[\"other_amt\"] = county_donors[\"other_amt\"]\n",
    "            break\n",
    "\n",
    "    #TODO set the unemployment data\n",
    "    \"\"\"  \n",
    "    unemployment = unemployment_df[(unemployment_df[\"County\"] == c) & (unemployment_df[\"Stabr\"] == state)]\n",
    "    unemployment_col = \"Unemployment_rate_\" + str(election_yr)\n",
    "    votes_dict[\"POPPCT_URBAN\"] = pd.to_numeric(unemployment[\"POPPCT_URBAN\"].values[0])\n",
    "    votes_dict[unemployment_col] = unemployment[unemployment_col].values[0]\n",
    "    votes_dict[\"POPDEN_URBAN\"] = unemployment[\"POPDEN_URBAN\"].values[0]\n",
    "    votes_dict[\"POPPCT_RURAL\"] = unemployment[\"POPPCT_RURAL\"].values[0]\n",
    "    votes_dict[\"POPDEN_RURAL\"] = unemployment[\"POPDEN_RURAL\"].values[0]\n",
    "    \"\"\" \n",
    "    return votes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the Linear Regression Structured Machine Learning\n",
    "def state_sml(state_tuple, state, election_yr, unemployment_df, education_df, birth_death_df):\n",
    "    #TODO merge together relevant info for county from unemployment_df, education_df, birth_death_df\n",
    "    counties_votes_dicts = state_tuple[0]    \n",
    "    donor_dicts = state_tuple[1]\n",
    "    \n",
    "    #Loop through each election year county dict\n",
    "    for i in range(0, len(counties_votes_dicts)):\n",
    "        #Select the corresponding counties/votes and donor info for that election yr\n",
    "        counties_votes_dict = counties_votes_dicts[i]\n",
    "        donor_dict = donor_dicts[i]\n",
    "        \n",
    "        county_dict = {}\n",
    "        #Loop through all the votes organized by county\n",
    "        for c in counties_votes_dict:\n",
    "            #Kepp unique dict of counties\n",
    "            if c not in county_dict:\n",
    "                county_dict[c] = {}\n",
    "            #Get the number of votes by county\n",
    "            county_votes = counties_votes_dict[c]\n",
    "            #Update the vote dict\n",
    "            votes_dict = set_votes_dict(county_dict[c], county_votes, donor_dict)\n",
    "            #Update the county dict with the updated votes dict\n",
    "            county_dict[c] = votes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the neural network model on the counties of votes and donors\n",
    "def run_nn_print(counties_votes_dict, donor_dict):\n",
    "    if dmdebug==True:\n",
    "        print(\"Running \" + run_nn_print.__name__ + \"()...\\n\")\n",
    "        \n",
    "    print(\"run_nn_print\")\n",
    "    print(counties_votes_dict)\n",
    "    print(donor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the neural network model on the counties of votes and donors\n",
    "def run_nn(counties_votes_dict, donor_dict):\n",
    "    if dmdebug==True:\n",
    "        print(\"Running \" + run_nn.__name__ + \"()...\\n\")\n",
    "        \n",
    "    #Neural Networking Code\n",
    "    nn_df = pd.DataFrame(counties_votes_dict)\n",
    "    #nn_df = pd.to_numeric(counties_votes_dict)\n",
    "    \n",
    "    if dmdebug==True:\n",
    "        print(\"nn_df_contents:\\n \" + str(nn_df) + \"\\nEND nn_df_contents\")\n",
    " \n",
    "    ### Change data type and generate our categorical variable list\n",
    "    votes_mi_cat = nn_df.dtypes[nn_df.dtypes == \"object\"].index.tolist()\n",
    "    #votes_mi_num = nn_df.dtypes[nn_df.dtypes != \"object\"].index.tolist()\n",
    "    \n",
    "    if dmdebug==True:\n",
    "        print(\"nn_df[votes_mi_cat]_contents:\\n \" + str(nn_df[votes_mi_cat]) + \"\\nEND nn_df[votes_mi_cat]_contents\")\n",
    "    \n",
    "    # Check the number of unique values in each column\n",
    "    nn_df[votes_mi_cat].nunique()\n",
    "    \n",
    "    # Create a OneHotEncoder instance\n",
    "    #enc = OneHotEncoder(sparse=False)\n",
    "    ### creating instance of labelencoder\n",
    "    le = LabelEncoder()\n",
    "    # Assigning numerical values and storing in another column\n",
    "    votesmi_encoded_df = nn_df.apply(le.fit_transform)\n",
    "    votesmi_encoded_df\n",
    "    \n",
    "    \n",
    "    # Fit and transform the OneHotEncoder using the categorical variable list\n",
    "    #encode_df = pd.DataFrame(enc.fit_transform(nn_df[votes_mi_cat]))\n",
    "\n",
    "    # Add the encoded variable names to the DataFrame\n",
    "    #encode_df.columns = enc.get_feature_names(votes_mi_cat)\n",
    "    #encode_df.head()\n",
    "    X = votesmi_encoded_df\n",
    "    X.columns.to_list()\n",
    "    X = X.fillna(0)\n",
    "    X.isnull().sum().values.any()\n",
    "    \n",
    "    # Create a y series from Is Successful column\n",
    "    y = X[\"transaction_amt\"]\n",
    "    \n",
    "    # Split X and y into training and testing sets\n",
    "    X = X.drop(\"transaction_amt\", axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "    \n",
    "    # Create a StandardScaler instance\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the StandardScaler\n",
    "    X_scaler = scaler.fit(X_train)\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    ###completeadjusted code###\n",
    "    \n",
    "    # Define the model - deep neural net\n",
    "    number_input_features = len(X_train[0])\n",
    "    hidden_nodes_layer1 =  8\n",
    "    hidden_nodes_layer2 = 5\n",
    "\n",
    "    nn = tf.keras.models.Sequential()\n",
    "\n",
    "    # First hidden layer\n",
    "    nn.add(\n",
    "        tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    "    )\n",
    "\n",
    "    # Second hidden layer\n",
    "    nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "    # Output layer\n",
    "    nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Check the structure of the model\n",
    "    nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given each county vote distribiton and donor distribution, run neural networks.\n",
    "def state_nn(state_tuple):\n",
    "    if dmdebug==True:\n",
    "        print(\"Running \" + state_nn.__name__ + \"()...\\n\")\n",
    "   \n",
    "    counties_votes_dicts = state_tuple[0]\n",
    "    donor_dicts = state_tuple[1]\n",
    "    \n",
    "    for i in range(0, len(counties_votes_dicts)):\n",
    "        counties_votes_dict = counties_votes_dicts[i]\n",
    "        donor_dict = donor_dicts[i]\n",
    "        #TODO enable the nn function, requires a DF\n",
    "        run_nn(counties_votes_dict, donor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running get_votes_intervals()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "Running vote_distribution()...\n",
      "\n",
      "     cmte_id                    cmte_nm cmte_tp cmte_city cmte_st cmte_zip  \\\n",
      "0  c00280453  JOHN SHADEGG FOR CONGRESS       H  PHOENIX,      AZ    85064   \n",
      "1  c00280453  JOHN SHADEGG FOR CONGRESS       H  PHOENIX,      AZ    85064   \n",
      "2  c00280453  JOHN SHADEGG FOR CONGRESS       H  PHOENIX,      AZ    85064   \n",
      "3  c00280453  JOHN SHADEGG FOR CONGRESS       H  PHOENIX,      AZ    85064   \n",
      "4  c00280453  JOHN SHADEGG FOR CONGRESS       H  PHOENIX,      AZ    85064   \n",
      "\n",
      "  cmte_dsgn cmte_pty_affiliation org_tp connected_org_nm  ... occupation  \\\n",
      "0         P                  REP   None             None  ...       None   \n",
      "1         P                  REP   None             None  ...       None   \n",
      "2         P                  REP   None             None  ...       None   \n",
      "3         P                  REP   None             None  ...       None   \n",
      "4         P                  REP   None             None  ...       None   \n",
      "\n",
      "   transaction_dt transaction_amt other_id tran_id file_num  memo_cd  \\\n",
      "0      1999-04-29             250     None    None     None     None   \n",
      "1      1999-05-11             200     None    None     None     None   \n",
      "2      1999-04-16            1000     None    None     None     None   \n",
      "3      1999-03-12             250     None    None     None     None   \n",
      "4      1999-04-27             200     None    None     None     None   \n",
      "\n",
      "  memo_text               sub id       party  \n",
      "0      None  3061920110006790000  republican  \n",
      "1      None  3061920110006790000  republican  \n",
      "2      None  3061920110006790000  republican  \n",
      "3      None  3061920110006790000  republican  \n",
      "4      None  3061920110006790000  republican  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "Y head\n",
      "[2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3\n",
      " 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1\n",
      " 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0\n",
      " 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0\n",
      " 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1\n",
      " 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1\n",
      " 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6\n",
      " 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 0 0 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5]\n",
      "X head\n",
      "[[ 2  3  0  1  0  0]\n",
      " [ 2  4  0  6 17  0]\n",
      " [ 2  6  0 14 19  0]\n",
      " ...\n",
      " [ 0  6  0 15  1  0]\n",
      " [ 0  6  0 16 10  0]\n",
      " [ 0  6  0 15  1  0]]\n",
      "Test Vals!!\n",
      "[[ 2  3  0  1  0  0]\n",
      " [ 2  4  0  6 17  0]\n",
      " [ 2  6  0 14 19  0]\n",
      " ...\n",
      " [ 0  6  0 15  1  0]\n",
      " [ 0  6  0 16 10  0]\n",
      " [ 0  6  0 15  1  0]]\n",
      "[2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3\n",
      " 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1\n",
      " 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0\n",
      " 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0\n",
      " 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1\n",
      " 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1\n",
      " 1 0 0 0 0 1 0 6 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6\n",
      " 1 0 2 1 0 2 1 2 0 3 0 2 1 1 2 5 0 1 3 0 1 1 0 0 0 0 1 0 6 1 0 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 0 0 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5]\n",
      "[4.31606246 4.3271945  4.32017877 5.75647779 4.28471263 4.33943342\n",
      " 5.0524316  4.28471263 5.76851508 4.31715136 4.33118281 4.2897342\n",
      " 4.28471263 4.32761095 4.29475577 4.34550271 4.28772558 4.34742452\n",
      " 4.33943342 4.32456907 4.3574966  4.31756781 4.28471263 5.75647779\n",
      " 4.33260358 5.75647779 4.3574966  4.33118281 4.32017877 4.26634877\n",
      " 4.34550271 4.28471263 4.28471263 5.0524316  4.33620438 4.28471263\n",
      " 3.62558786 4.34550271 4.31715136 4.32458354 4.35149966 4.26634877\n",
      " 4.33260358 4.28471263 4.3271945  4.34742452 4.34742452 4.33118281\n",
      " 4.33620438 4.33118281 4.26634877 4.34550271 4.32761095 5.75647779\n",
      " 4.34742452 4.32313383 4.28471263 4.32458354 4.219922   4.32456907\n",
      " 4.31756781 4.2897342  4.31606246 4.26634877 4.32313383 4.3574966\n",
      " 4.28772558 4.32017877 5.76851508 4.2897342  4.32456907 4.31713689\n",
      " 5.75647779 4.31713689 4.28471263 4.31756781 4.31713689 5.03058047\n",
      " 4.29475577 4.26634877]\n",
      "Prediction!!\n",
      "Confusion Matrix!!\n",
      "[[ 0  0  0  1 23  0  0]\n",
      " [ 0  0  0  0 22  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  3  5  0]\n",
      " [ 0  0  0  0  4  0  0]]\n",
      "Classification Report!!\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        24\n",
      "           1       0.00      0.00      0.00        22\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         5\n",
      "           5       0.50      0.62      0.56         8\n",
      "           6       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.06        80\n",
      "   macro avg       0.07      0.09      0.08        80\n",
      "weighted avg       0.05      0.06      0.06        80\n",
      "\n",
      "Y head\n",
      "[ 0 17 19  6 16  3 14  7 14 13  9  5 16 16 11 16 20 16  7 13 18  8  4  4\n",
      "  2  4  4 16 16  0 17 19  6 16  3 14  7 14 13  9  5 16 16 11 16 20 16  7\n",
      " 13 18  8  4  4  2  4  4 16 16  0 17 19  6 16  3 14  7 14 13  9  5 16 16\n",
      " 11 16 20 16  7 13 18  8  4  4  2  4  4 16 16  0 17 19  6 16  3 14  7 14\n",
      " 13  9  5 16 16 11 16 20 16  7 13 18  8  4  4  2  4  4 16 16  0 17 19  6\n",
      " 16  3 14  7 14 13  9  5 16 16 11 16 20 16  7 13 18  8  4  4  2  4  4 16\n",
      " 16  0 17 19  6 16  3 14  7 14 13  9  5 16 16 11 16 20 16  7 13 18  8  4\n",
      "  4  2  4  4 16 16  0 17 19  6 16  3 14  7 14 13  9  5 16 16 11 16 20 16\n",
      "  7 13 18  8  4  4  2  4  4 16 16  0 17 19  6 16  3 14  7 14 13  9  5 16\n",
      " 16 11 16 20 16  7 13 18  8  4  4  2  4  4 16 16  0 17 19  6 16  3 14  7\n",
      " 14 13  9  5 16 16 11 16 20 16  7 13 18  8  4  4  2  4  4 16 16  0 17 19\n",
      "  6 16  3 14  7 14 13  9  5 16 16 11 16 20 16  7 13 18  8  4  4  2  4  4\n",
      " 16 16 15 19 15 19 15 19 15 19 15 19 15 19 12 12 10  1 10  1 10  1 10  1\n",
      " 10  1 10  1 10  1 10  1]\n",
      "X head\n",
      "[[ 2  3  0  1  0]\n",
      " [ 2  4  0  6  0]\n",
      " [ 2  6  0 14  0]\n",
      " ...\n",
      " [ 0  6  0 15  0]\n",
      " [ 0  6  0 16  0]\n",
      " [ 0  6  0 15  0]]\n",
      "Test Vals!!\n",
      "[[ 2  3  0  1  0]\n",
      " [ 2  4  0  6  0]\n",
      " [ 2  6  0 14  0]\n",
      " ...\n",
      " [ 0  6  0 15  0]\n",
      " [ 0  6  0 16  0]\n",
      " [ 0  6  0 15  0]]\n",
      "[ 0 17 19  6 16  3 14  7 14 13  9  5 16 16 11 16 20 16  7 13 18  8  4  4\n",
      "  2  4  4 16 16  0 17 19  6 16  3 14  7 14 13  9  5 16 16 11 16 20 16  7\n",
      " 13 18  8  4  4  2  4  4 16 16  0 17 19  6 16  3 14  7 14 13  9  5 16 16\n",
      " 11 16 20 16  7 13 18  8  4  4  2  4  4 16 16  0 17 19  6 16  3 14  7 14\n",
      " 13  9  5 16 16 11 16 20 16  7 13 18  8  4  4  2  4  4 16 16  0 17 19  6\n",
      " 16  3 14  7 14 13  9  5 16 16 11 16 20 16  7 13 18  8  4  4  2  4  4 16\n",
      " 16  0 17 19  6 16  3 14  7 14 13  9  5 16 16 11 16 20 16  7 13 18  8  4\n",
      "  4  2  4  4 16 16  0 17 19  6 16  3 14  7 14 13  9  5 16 16 11 16 20 16\n",
      "  7 13 18  8  4  4  2  4  4 16 16  0 17 19  6 16  3 14  7 14 13  9  5 16\n",
      " 16 11 16 20 16  7 13 18  8  4  4  2  4  4 16 16  0 17 19  6 16  3 14  7\n",
      " 14 13  9  5 16 16 11 16 20 16  7 13 18  8  4  4  2  4  4 16 16  0 17 19\n",
      "  6 16  3 14  7 14 13  9  5 16 16 11 16 20 16  7 13 18  8  4  4  2  4  4\n",
      " 16 16 15 19 15 19 15 19 15 19 15 19 15 19 12 12 10  1 10  1 10  1 10  1\n",
      " 10  1 10  1 10  1 10  1]\n",
      "[9.32597406 9.35650374 9.34026558 8.47894294 9.19314873 9.52180541\n",
      " 9.04908868 9.19314873 8.4951811  9.35650374 9.38898008 9.19314873\n",
      " 9.19314873 9.38995341 9.19314873 9.47309091 9.19314873 9.57051991\n",
      " 9.52180541 9.42242975 9.53804358 9.38995341 9.19314873 8.47894294\n",
      " 9.42242975 8.47894294 9.53804358 9.38898008 9.34026558 9.01355556\n",
      " 9.47309091 9.19314873 9.19314873 9.04908868 9.38898008 9.19314873\n",
      " 9.94386098 9.47309091 9.35650374 9.40619158 9.50556724 9.01355556\n",
      " 9.42242975 9.19314873 9.35650374 9.57051991 9.57051991 9.38898008\n",
      " 9.38898008 9.38898008 9.01355556 9.47309091 9.38995341 8.47894294\n",
      " 9.57051991 9.40521825 9.19314873 9.40619158 8.76900971 9.42242975\n",
      " 9.38995341 9.19314873 9.32597406 9.01355556 9.40521825 9.53804358\n",
      " 9.19314873 9.34026558 8.4951811  9.19314873 9.42242975 9.37274191\n",
      " 8.47894294 9.37274191 9.19314873 9.38995341 9.37274191 8.90099851\n",
      " 9.19314873 9.01355556]\n",
      "Prediction!!\n",
      "Confusion Matrix!!\n",
      "[[ 0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  8  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "Classification Report!!\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         9\n",
      "           5       0.00      0.00      0.00         4\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.00      0.00      0.00         5\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       0.03      1.00      0.05         2\n",
      "          10       0.00      0.00      0.00         5\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.00      0.00      0.00         4\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00        15\n",
      "          17       0.00      0.00      0.00         3\n",
      "          18       0.00      0.00      0.00         3\n",
      "          19       0.00      0.00      0.00         4\n",
      "          20       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.03        80\n",
      "   macro avg       0.00      0.05      0.00        80\n",
      "weighted avg       0.00      0.03      0.00        80\n",
      "\n",
      "Y head\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "X head\n",
      "[[ 2  3  0  1]\n",
      " [ 2  4  0  6]\n",
      " [ 2  6  0 14]\n",
      " ...\n",
      " [ 0  6  0 15]\n",
      " [ 0  6  0 16]\n",
      " [ 0  6  0 15]]\n",
      "Test Vals!!\n",
      "[[ 2  3  0  1]\n",
      " [ 2  4  0  6]\n",
      " [ 2  6  0 14]\n",
      " ...\n",
      " [ 0  6  0 15]\n",
      " [ 0  6  0 16]\n",
      " [ 0  6  0 15]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Prediction!!\n",
      "Confusion Matrix!!\n",
      "[[80]]\n",
      "Classification Report!!\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        80\n",
      "\n",
      "    accuracy                           1.00        80\n",
      "   macro avg       1.00      1.00      1.00        80\n",
      "weighted avg       1.00      1.00      1.00        80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\dcohen\\.conda\\envs\\PythonData\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running state_nn()...\n",
      "\n",
      "Running run_nn()...\n",
      "\n",
      "nn_df_contents:\n",
      "                   Apache    Cochise  Coconino       Gila    Graham   Greenlee  \\\n",
      "blue_votes         13025      13360     20280       7700      3355       1216   \n",
      "red_votes           5947      18180     17562       9158      6007       1619   \n",
      "other_votes          484       1701      3041        878       302        125   \n",
      "total_votes        19456      33241     40883      17736      9664       2960   \n",
      "percent_blue    0.669459   0.401913   0.49605   0.434145  0.347165   0.410811   \n",
      "percent_red     0.305664   0.546915  0.429567   0.516351  0.621585   0.546959   \n",
      "percent_other  0.0248766  0.0511717  0.074383  0.0495038   0.03125  0.0422297   \n",
      "county            Apache    Cochise  Coconino       Gila    Graham   Greenlee   \n",
      "state                 AZ         AZ        AZ         AZ        AZ         AZ   \n",
      "\n",
      "                  La Paz  Maricopa     Mohave     Navajo       Pima  \\\n",
      "blue_votes          1769    386683      17470      11794     147688   \n",
      "red_votes           2543    479967      24386      12386     124579   \n",
      "other_votes          171     35049       2285        967      15373   \n",
      "total_votes         4483    901699      44141      25147     287640   \n",
      "percent_blue    0.394602  0.428838   0.395777   0.469002   0.513447   \n",
      "percent_red     0.567254  0.532292   0.552457   0.492544   0.433107   \n",
      "percent_other  0.0381441   0.03887  0.0517659  0.0384539  0.0534453   \n",
      "county            La Paz  Maricopa     Mohave     Navajo       Pima   \n",
      "state                 AZ        AZ         AZ         AZ         AZ   \n",
      "\n",
      "                   Pinal  Santa Cruz    Yavapai       Yuma  \n",
      "blue_votes         19650        5233      24063      12055  \n",
      "red_votes          20122        3344      40144      15708  \n",
      "other_votes         1518         316       4021        889  \n",
      "total_votes        41290        8893      68228      28652  \n",
      "percent_blue    0.475902     0.58844   0.352685   0.420739  \n",
      "percent_red     0.487333    0.376026    0.58838   0.548234  \n",
      "percent_other  0.0367643   0.0355336  0.0589347  0.0310275  \n",
      "county             Pinal  Santa Cruz    Yavapai       Yuma  \n",
      "state                 AZ          AZ         AZ         AZ  \n",
      "END nn_df_contents\n",
      "nn_df[votes_mi_cat]_contents:\n",
      "                   Apache    Cochise  Coconino       Gila    Graham   Greenlee  \\\n",
      "blue_votes         13025      13360     20280       7700      3355       1216   \n",
      "red_votes           5947      18180     17562       9158      6007       1619   \n",
      "other_votes          484       1701      3041        878       302        125   \n",
      "total_votes        19456      33241     40883      17736      9664       2960   \n",
      "percent_blue    0.669459   0.401913   0.49605   0.434145  0.347165   0.410811   \n",
      "percent_red     0.305664   0.546915  0.429567   0.516351  0.621585   0.546959   \n",
      "percent_other  0.0248766  0.0511717  0.074383  0.0495038   0.03125  0.0422297   \n",
      "county            Apache    Cochise  Coconino       Gila    Graham   Greenlee   \n",
      "state                 AZ         AZ        AZ         AZ        AZ         AZ   \n",
      "\n",
      "                  La Paz  Maricopa     Mohave     Navajo       Pima  \\\n",
      "blue_votes          1769    386683      17470      11794     147688   \n",
      "red_votes           2543    479967      24386      12386     124579   \n",
      "other_votes          171     35049       2285        967      15373   \n",
      "total_votes         4483    901699      44141      25147     287640   \n",
      "percent_blue    0.394602  0.428838   0.395777   0.469002   0.513447   \n",
      "percent_red     0.567254  0.532292   0.552457   0.492544   0.433107   \n",
      "percent_other  0.0381441   0.03887  0.0517659  0.0384539  0.0534453   \n",
      "county            La Paz  Maricopa     Mohave     Navajo       Pima   \n",
      "state                 AZ        AZ         AZ         AZ         AZ   \n",
      "\n",
      "                   Pinal  Santa Cruz    Yavapai       Yuma  \n",
      "blue_votes         19650        5233      24063      12055  \n",
      "red_votes          20122        3344      40144      15708  \n",
      "other_votes         1518         316       4021        889  \n",
      "total_votes        41290        8893      68228      28652  \n",
      "percent_blue    0.475902     0.58844   0.352685   0.420739  \n",
      "percent_red     0.487333    0.376026    0.58838   0.548234  \n",
      "percent_other  0.0367643   0.0355336  0.0589347  0.0310275  \n",
      "county             Pinal  Santa Cruz    Yavapai       Yuma  \n",
      "state                 AZ          AZ         AZ         AZ  \n",
      "END nn_df[votes_mi_cat]_contents\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument must be a string or number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\PythonData\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\u001b[0m in \u001b[0;36m_encode\u001b[1;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\PythonData\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\u001b[0m in \u001b[0;36m_encode_python\u001b[1;34m(values, uniques, encode)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muniques\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'numpy.ndarray' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-e19c65de81d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Run the main loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-a8239a8d76e3>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m#TODO enable the neural networking code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mstate_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m#TODO: Now with all states donations and voting results aggregated, predict the number of votes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-73ba0c990fc4>\u001b[0m in \u001b[0;36mstate_nn\u001b[1;34m(state_tuple)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdonor_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdonor_dicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#TODO enable the nn function, requires a DF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mrun_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounties_votes_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdonor_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-e1fe1d9af782>\u001b[0m in \u001b[0;36mrun_nn\u001b[1;34m(counties_votes_dict, donor_dict)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Fit and transform the OneHotEncoder using the categorical variable list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mencode_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvotes_mi_cat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Add the encoded variable names to the DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\PythonData\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    370\u001b[0m         \"\"\"\n\u001b[0;32m    371\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\PythonData\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\PythonData\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \"\"\"\n\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_idx_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_drop_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\PythonData\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, handle_unknown)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mXi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 \u001b[0mcats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mcats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\PythonData\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\u001b[0m in \u001b[0;36m_encode\u001b[1;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"argument must be a string or number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument must be a string or number"
     ]
    }
   ],
   "source": [
    "#Run the main loop\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
